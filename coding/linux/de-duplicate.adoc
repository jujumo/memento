= image:bash_icon.svg["BASH", width=64px] Search and destroy file duplicates
:experimental:
:toc:

This tutorial explains how to use plain gnu tools to find and remove duplicate files.
It only rely on bash command and gnu tools, such as: parallel, sha256sum and other bash commands.

== install required tools

`cat`, `find`, `sed`, `uniq`, `sponge`, `grep`, 'tree` and `sort` should be installed.

=== To insatll `sha256sum` :

[source,bash]
$> sudo apt-get install sha256sum

=== To install parallel: 

link:parallel.adoc[parallel.adoc]

== building `fingerprints.txt` index file using hash

To search duplicates, I rely on sha256sum hash functions. 
So first thing to do, is to compute the hash for all files.
It may take a while.

.terminal
[source,bash]
----
# compute hash for files
$> find ./ -type f ! -wholename "*/@eaDir/*" ! -name "Thumbs.db" | parallel "sha256sum {} >> fingerprints.txt"
# you can repeat the hash process with multiple path for find, as long as root path is the same (or use absolute paths).

# optionally exclude some files a posteriori
$> grep -v "selection" fingerprints.txt | sponge fingerprints.txt

# At the end, make sure you have no duplicate entries that can pollute the following processes.
# duplicate entries might happens if you apply twice same command.
$> sort --unique fingerprints.txt -o fingerprints.txt  # heals remove single file duplication in case of multiple runs
----

`fingerprints.txt` should looks like:

.fingerprints.txt
----
0191a0f48a8a0d4f1db3e906c54d4f0b815da7dd7eea3abb9682265b8aef5e8c  ./mydir/tmp.8UTz5kGOiZ
1149788a2cc8097bf17bbcbd9a5aa56d08e4b4ea669279259fe40a5a08d37970  ./mydir/tmp.Q6bW1mVC4J
5b07fc1686009a66c650982c0a6363f69301c5369f2dd87ec930eee583aa2ebc  ./mydir/tmp.WI4RxZeymC
6a8eaa536cc65fc6490e6b743e722647037b2e57e6461c875a76539685e167a5  ./mydir/tmp.MjEENkA9rM
78fb97104bfc022e2d3f8bd40193ff0ede37fd3a398a0e2ac0947fbfc6627aaf  ./mydir/tmp.FBINu8gnGL
87c67bbc3c3b16f7ad8f8db6d912df5739efea9be76218f5d6a9d38524b34724  ./mydir/tmp.dzCsVWATqC
8af7a327a438d22a8158eac434c37e16fd70ffa326a2d520f52b819dc07a1b48  ./mydir/tmp.sgprKjAw1B
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.foNntc262d
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.H24yD9WWzP
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.Xdr3qwyAu9
98bd06465977efdb4026b198c0f81dd4565754c5705cb57e82c2733b452464bd  ./mydir/tmp.nYU8iWL1MY
9cd05f25f126936ae1e6ac487021033fd8c02760c48eac35af07a66755e4d684  ./mydir/tmp.E1dIRUDPLf
abf61dc38ed225c2c2d71afc6f093e120289c1ca5bdb10b70b195b6891560496  ./mydir/tmp.BlgDxx3yAj
d38407d55361bc28decb6e8d5f42587aa04651772342ceb906fd08d687ff5653  ./mydir/tmp.uMXMd32oLk
e458588262911df9a6bf9be66bc93955db4a396b42c9d180d7387eafa29cf987  ./mydir/tmp.kpg3eqAzUi
e458588262911df9a6bf9be66bc93955db4a396b42c9d180d7387eafa29cf987  ./mydir/tmp.zx5eYf4cgK
ee5954355d52d5fdde425ab37adb2247cad3c8492966005631ee04fda2d5f188  ./mydir/tmp.47gcuzDvp2
ee5954355d52d5fdde425ab37adb2247cad3c8492966005631ee04fda2d5f188  ./mydir/tmp.zXo79fQtSm
f2d922da9e89fa59ee4561cadf2b0b809e18ac01360e8ea53d67d643e4e645bb  ./mydir/tmp.ziiDYzWAG2
----


== build table of duplicate files in `duplicates.txt`

Once, `fingerprints.txt` index file is built, you can look for duplication.


.terminal
[source,bash]
----
$> cat fingerprints.txt | sort -k1 | uniq -D --check-chars=65 > duplicates.txt
----

.duplicates.txt
----
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.foNntc262d
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.H24yD9WWzP
8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.Xdr3qwyAu9
e458588262911df9a6bf9be66bc93955db4a396b42c9d180d7387eafa29cf987  ./mydir/tmp.kpg3eqAzUi
e458588262911df9a6bf9be66bc93955db4a396b42c9d180d7387eafa29cf987  ./mydir/tmp.zx5eYf4cgK
ee5954355d52d5fdde425ab37adb2247cad3c8492966005631ee04fda2d5f188  ./mydir/tmp.47gcuzDvp2
ee5954355d52d5fdde425ab37adb2247cad3c8492966005631ee04fda2d5f188  ./mydir/tmp.zXo79fQtSm
----

You can inspect the number of duplicate files using the following:

----
$> cat duplicates.txt | sort -k1 | uniq --repeated --check-chars=65 --count | sort -r -k1 | head
      3 8f04793f9be7bb502d33cec17a9aad557d84f0a194c48bc7530c845a73c541eb  ./mydir/tmp.foNntc262d
      2 ee5954355d52d5fdde425ab37adb2247cad3c8492966005631ee04fda2d5f188  ./mydir/tmp.47gcuzDvp2
      2 e458588262911df9a6bf9be66bc93955db4a396b42c9d180d7387eafa29cf987  ./mydir/tmp.kpg3eqAzUi
----


== suppression

.terminal
[source,bash]
----
$> cat duplicates.txt | sed 's/^.\{66\}//' > suppression.txt
$> cat suppression.txt | grep -i "0-temp"  | sponge suppression.txt

# remove dupliacte
$> mkdir trash
$> parallel mv {} trash/{/} :::: suppression.txt
$> du -csh trash  # see how much space is gained
----

== clean up and update 

.terminal
[source,bash]
----
$> rm -rf trash suppression.txt duplicates.txt

# filter out files that does not exists anymore
$> cat fingerprints.txt | parallel --plus  "[[ -f {= s/^.{66}// =} ]] && echo {}" | sponge fingerprints.txt
----